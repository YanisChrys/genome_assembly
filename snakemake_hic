# Genome Assembly: Overall workflow
#import csv
#import warnings
#import pysam
import re
import os

# todo : look into using kraken

# genome assembly pipeline
# to run per chromosome, meaning there is only 1 input each time
# must provide the appropriate setting that you need to use
# and a prefix for the file

configfile: "config.yaml"

# partition parts of each job
# number of threads to use
# log level
# database to use for BUSCO
# if error add zip after string?
# INPUT_SUBREADS=f"{READ_PATH}""/{subreadfile}.subreads.bam" 
# INPUT_HIFI=f"{READ_FILE}""/{hififile}.vcf.gz" 

############  SET UP PATH TO INPUT FILES AND FILE NAME STEMS AND SAVE THEM AS WILDCARDS  ############
# make sure to get the location and name of the input file and notify the user if an input is missing
# subreads and fasta hifi reads must be a single file.
# hic reads need to be 2 files, R1 and R2

SUBREADS=config["SUBREADS"]
HIFI=config["HIFI"]
HIC1=config["HIC1"]
HIC2=config["HIC2"]
ENZYMES=config["ENZYMES"]

#subreads and fasta hifi reads must be a single file.
#TODO add a way to handle complex file names
#save anything after split('.')[0] as variable extension
if SUBREADS: 
    #READ_PATH=os.path.dirname(SUBREADS)
    SAMPLE_LABEL=os.path.basename(SUBREADS).split('.')[0] 
elif HIFI: 
    #    print("Subreads not provided.")
    #READ_PATH=os.path.dirname(HIFI)
    SAMPLE_LABEL=os.path.basename(HIFI).split('.')[0]
#elif not SUBREADS and not HIFI: 
#    print("No appropriate input data provided.")


# if HIC1 and HIC2:
#     HIC1_PATH=os.path.dirname(HIC1)
#     HIC1_FILE=os.path.basename(HIC1).split('.')[0]
#     HIC2_PATH=os.path.dirname(HIC2)
#     HIC2_FILE=os.path.basename(HIC2).split('.')[0]
# #elif (HIC1 and not HIC2) or (HIC2 and not HIC1):
#    print("Only 1 HiC file provided")
#elif not HIC1 and not HIC2:
#    print("No HiC files provided.")


############  RULE ALL FILES ############

# add output files to rule all input so that the rules containing them will be included in the workflow.
# create an empty list and append output file names to it that will then be used as the input of the "all" rule
# if a pipeline rule is defined and its output isn't found on the all rule, it simply won't run

# by default, and until another assembly program is added, hifiasm will always run

STOP_AT_HIFIASM=config["STOP_AT_HIFIASM"]
DO_FASTK=config["DO_FASTK"]
DO_BUSCO=config["DO_BUSCO"]
DO_HIC=config["DO_HIC"]
DO_CCS=config["DO_CCS"]
DO_DEDUP=config["DO_DEDUP"]
CHUNK_NMB=list(range(1,config["CHUNKS"]+1))

#remove potential special characters and dots from prefix but keep underscores
FILE_PREFIX=re.sub('[^a-zA-Z0-9 \_]', '', config["FILE_PREFIX"])


# need to constrain wildcards so that smk can better resolve ambiguous wildcards and files
# this would not be necessary if we provided a regex for our input files but here we want to
# allow for completely unknown file names
wildcard_constraints:
    chunknumber="[0-9]+",
    hic="hic[0-9]"

OUTPUT_FILES = []

if STOP_AT_HIFIASM:
    # make sure that if the user forgot to set all other routes to false that they don't run
    # then add the hifiasm output files to rule all, if we do other steps we wouldn't need these files in there
    DO_FASTK = False
    DO_BUSCO = False
    DO_HIC = False
    DO_DEDUP = False
    hifiasm1=expand("RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.a_ctgl2.fasta", prefix=FILE_PREFIX)
    hifiasm2=expand("RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.p_ctgl2.fasta", prefix=FILE_PREFIX)
    OUTPUT_FILES=OUTPUT_FILES + hifiasm1 + hifiasm2
if DO_FASTK:
    katgc1=expand("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.st.png", prefix=FILE_PREFIX)
    katgc2=expand("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.fi.png", prefix=FILE_PREFIX)
    katgc3=expand("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.ln.png", prefix=FILE_PREFIX)
    ploidyplot1=expand("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.pdf", prefix=FILE_PREFIX)
    genescope1=expand("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/genescopeFK/{prefix}_progress.txt", prefix=FILE_PREFIX)
    seqstats=expand("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/STATISTICS/{prefix}.ccs.readStats.txt", prefix=FILE_PREFIX)
    OUTPUT_FILES=OUTPUT_FILES + katgc1 + katgc2 + katgc3 + ploidyplot1 + genescope1 + seqstats
if DO_BUSCO:
    buscoout=expand("RESULTS/GENOME_ASSEMBLY/BUSCO/{prefix}_busco_missing.tsv", prefix=FILE_PREFIX)
    OUTPUT_FILES=OUTPUT_FILES + buscoout
# turn path string to list and add to output
if DO_HIC:
    hicresorted1=["RESULTS/GENOME_ASSEMBLY/HIC/SALSA2/scaffolds_FINAL.fasta","RESULTS/GENOME_ASSEMBLY/HIC/SALSA2/scaffolds_FINAL.agp"]
    hicfiles=expand("DATA/{hic}.fastq.gz",hic=["hic1","hic2"])
    combined1=expand("RESULTS/GENOME_ASSEMBLY/HIC/RAW/{prefix}_{hic}.bam",hic=["hic1","hic2"],prefix=FILE_PREFIX)
    filt1=expand("RESULTS/GENOME_ASSEMBLY/HIC/FILTERED/{prefix}_{hic}_filtered.bam",hic=["hic1","hic2"],prefix=FILE_PREFIX)
    OUTPUT_FILES=OUTPUT_FILES + hicresorted1 + hicfiles + combined1 + filt1
if DO_DEDUP:
    getseqs1=expand("RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}_seqs_purged.hap.fa", prefix=FILE_PREFIX)
    OUTPUT_FILES=OUTPUT_FILES + getseqs1
if DO_CCS:
    checkfinished="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/CCS_PACBIO/MERGED/check.txt"
    ccsreads=expand("DATA/{prefix}.subreads.bam",prefix=FILE_PREFIX)
    ccsout=expand("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/CCS_PACBIO/{prefix}_{chunknumber}.ccs.bam",chunknumber=CHUNK_NMB, prefix=FILE_PREFIX)
    OUTPUT_FILES=OUTPUT_FILES + [checkfinished] + ccsout + ccsreads
elif not DO_CCS:
    hifireads1=expand("DATA/{prefix}.fastq.gz",prefix=FILE_PREFIX)
    OUTPUT_FILES=OUTPUT_FILES + hifireads1
############  RULE ALL  ############

#BRIDGE=['a_ctg','p_ctg','p_ctg','r_utg']
# Terminal node:
rule all:
    input:
        OUTPUT_FILES



rule CP_SUBREADS_TO_SYSTEM:
    input:
        config["SUBREADS"]
    output:
        expand("DATA/{prefix}.subreads.bam",prefix=FILE_PREFIX)
    shell: """
        cp {input} {output}
    """

# add temp to output after you're done testing
# if input is newer than output snakemake will rerun all the rules
rule CP_HIFI_TO_SYSTEM:
    input:
        config["HIFI"]
    output:
        expand("DATA/{prefix}.fastq.gz",prefix=FILE_PREFIX)
    shell: """
        cp {input} {output}
    """
rule CP_HIC1_TO_SYSTEM:
    input:
        config["HIC1"]
    output:
        "DATA/hic1.fastq.gz"
    shell: """
        cp {input} {output}
    """

rule CP_HIC2_TO_SYSTEM:
    input:
        config["HIC2"]
    output:
        "DATA/hic2.fastq.gz"
    shell: """
        cp {input} {output}
    """

############  PRIMARY PREPROCESSING MODULE  ############
############  IF READS ARE NOT CIRCULAR  ############

# run ccs to create circular hifi reads

rule RUN_CCS:
    input:
        "DATA/{prefix}.subreads.bam"
    output:
        bamo="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/CCS_PACBIO/{prefix}_{chunknumber}.ccs.bam",
        index="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/CCS_PACBIO/{prefix}_{chunknumber}.ccs.bam.pbi",
        metrics="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/CCS_PACBIO/{prefix}_{chunknumber}_metrics.json.gz", 
        rprt="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/CCS_PACBIO/{prefix}_{chunknumber}_report.txt"
    threads:
        config["CORES"]
    params:
        loglevel=config["LOGLEVEL"],
        chunk="{chunknumber}/%s" % config["CHUNKS"],
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/PREPROCESSING.CCS_PACBIO.{prefix}_{chunknumber}.ccs.log"
    shell: """
        ccs {input} {output.bamo} --num-threads {threads} --chunk {params.chunk} \
        --log-level {params.loglevel} --log-file {log} --report-file {output.rprt} --metrics-json {output.metrics}
    """

rule MERGE_CCS:
    input:
        expand("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/CCS_PACBIO/{prefix}_{chunknumber}.ccs.bam",chunknumber=CHUNK_NMB, prefix=FILE_PREFIX)
    output:
        touch("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/CCS_PACBIO/MERGED/{prefix}.merged.ccs.bam")
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/PREPROCESSING.CCS_PACBIO.{prefix}.merge.log"
    shell: """
        samtools merge -c -p -f -@{threads} {output} {input}
    """

rule INDEX_MERGED_CCS:
    input:
        "RESULTS/GENOME_ASSEMBLY/PREPROCESSING/CCS_PACBIO/MERGED/{prefix}.merged.ccs.bam"
    output:
        touch("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/CCS_PACBIO/MERGED/{prefix}.merged.ccs.bam.pbi")
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/PREPROCESSING.CCS_PACBIO.{prefix}.pbindex.log"
    shell: """
        pbindex {input}
    """

#convert bam to fasta format with bam2fasta if it was merged
#but we can create it as a fasta from the get-go, see rule 1

rule CONVERT_TO_FASTQ:
    input:
        bam="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/CCS_PACBIO/MERGED/{prefix}.merged.ccs.bam",
        pbi="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/CCS_PACBIO/MERGED/{prefix}.merged.ccs.bam.pbi"
    output:
        "DATA/{prefix}.fastq.gz"
    params:
        "DATA/{prefix}"
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/PREPROCESSING/CCS_PACBIO/{prefix}.bam2fastq.log"
    shell: """
        bam2fastq -o {params} {input.bam}
    """


############  INITIALIZE PRIMARY STATISTICAL MODULE AND TABULAR ARRAY ############

# seqkit can be used to get some initial read statistics from the CCS reads
# Do all (-a) statistics on the reads (quartiles of seq length, sum_gap, N50, etc)

rule SEQKIT_CCS_STATS:
    input:
        "DATA/{prefix}.fastq.gz"
    output:
        "RESULTS/GENOME_ASSEMBLY/PREPROCESSING/STATISTICS/{prefix}.ccs.readStats.txt"
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/PREPROCESSING.STATISTICS.{prefix}.seqkit.log"
    shell: """
        seqkit stats -a {input} > {output} 
    """

# FastK
# kmers are k-sized string chunks - here it is DNA
# FastK splits the DNA into kmers and counts how many timers it sees each one
# need to provide absolute path of the module's locations
rule FASTK_TABLE:
    input:
        "DATA/{prefix}.fastq.gz"
    output:
        ktab=touch("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.ktab"),
        hist=touch("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.hist")
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/PREPROCESSING.STATISTICS.{prefix}.fastk.log"
    params:
        out="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}",
        temp="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/tmp",
        chunk=config["CHUNKS"]
    # envmodules:
    #     "fastk/current"
    shell: """
        module load fastk/current
        FastK -v -t1 -T{threads} -k40 -N{params.out} -P{params.temp} {input} 
    """
# -PRESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/tmp DATA/phophogenome143410.fastq.gz DATA/phophogenome143410.fastq.gz
############  GENOME SIZE ESTIMATE WITH GENESCOPE.FK  ############

# Histex histograg=input for Genescope.FK
# Simplify FastK hist table
# h=frequencies to be displayed
# NOTE: consider exploring alterantive h values
rule HISTEX:
    input:
        "RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.hist"
    output:
        "RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.hist.txt"
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/PREPROCESSING.STATISTICS.{prefix}.histex.log"
    # envmodules:
    #     "fastk/current"
    shell: """
        module load fastk/current
        Histex -G {input} -h1000 > {output} 
    """

# Genescope.FK
# test if just saying (directory() would work)
#  plots describing genome properties such as genome size, heterozygosity, and repetitiveness
# it will fail if there isn't enough data
rule GENESCOPE_FK:
    input:
        "RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.hist.txt"
    output:
        progress=touch("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/genescopeFK/{prefix}_progress.txt")
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/PREPROCESSING.STATISTICS.{prefix}.GeneScopeFK.log"
    params:
        inputprefix="{prefix}",
        outfolder="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/genescopeFK",
        loglevel=config["LOGLEVEL"],
        chunk=config["CHUNKS"],
        kmersize=config["KMER"]
    # envmodules:
    #     "genescopefk/current",
    #     "fastk/current"
    shell: """
        module load genescopefk/current
        module load fastk/current
        GeneScopeFK.R -i {input} -o {params.outfolder} -k {params.kmersize} -n {params.inputprefix} 
    """

############  READ ANALYSIS  ############
##########  MERQURY.FK MODULE  ##########

# module path: /share/scientific_bin/merquryfk/current

# KatGC
# 3D heat map or contour map of the frequency of a k-mer versus its' GC content
# input=FastK ktab file (auto-detected)
rule KATGC:
    input:
        hist="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.hist",
        ktab="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.ktab"
    output:
        png1="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.st.png",
        png2="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.fi.png",
        png3="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.ln.png"
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/PREPROCESSING.STATISTICS.{prefix}.KatGC.log"
    envmodules:
        "merquryfk/current",
        "fastk/current"
    conda:
        "envs/merqury.yaml"
    shell: """
        # module load merquryfk/current
        # module load fastk/current
        cd RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/
        # source /share/scientific_bin/anaconda3/2022.05/etc/profile.d/conda.sh
        # conda activate merqury
        KatGC -T{threads} {wildcards.prefix} {wildcards.prefix}
    """

# PloidyPlot
# useful in case the ploidy of your sample is unknown
# input=FastK ktab file (auto-detected)
# verbose and keep the table it creates
# plot in pdf format
# Needs program Logex
rule PLOIDYPLOT:
    input:
        hist="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.hist",
        ktab="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.ktab"
    output:
        touch("RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/{prefix}.pdf")
    threads:
        workflow.cores    
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/PREPROCESSING.STATISTICS.{prefix}.PloidyPlot.log"
    params: 
        inp=lambda wildcards, output: output[0][:-4],
        out="RESULTS/GENOME_ASSEMBLY/PREPROCESSING/FASTKMERS/",
        loglevel=config["LOGLEVEL"],
        chunk=config["CHUNKS"]
    envmodules:
        "merquryfk/current",
        "fastk/current"
    shell: """
    #    module load merquryfk/current
    #    module load fastk/current
        PloidyPlot -T{threads} -v -pdf -o{params.out} {params.inp} 
    """
#PloidyPlot -vk -pdf -o"$(pwd)/" some_reads
 
############  GENOME ASSEMBLY WITH HIFIASM  ############

# HiFiasm
# also try -l0 and -l3 or others

# l: 0-3 level of purging
# primary: output a primary assembly and an alternate assembly
# o: prefix of output files 
# t: threads

# inspired by:
# https://snakemake-wrappers.readthedocs.io/en/latest/wrappers/hifiasm.html
# Input: list of files
# output: list of files that differ by their extension
# run hifiasm so that if hic is provided it is used (else [" {hic1} {hic2}"] string will be empty)
rule RUN_HIFIASM:
    input:
        "DATA/{prefix}.fastq.gz"
    output:
        "RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.p_ctg.gfa",
        "RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.a_ctg.gfa"
        # multiext("RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.",
        #     "a_ctg.gfa",
        #     "a_ctg.lowQ.bed",
        #     "a_ctg.noseq.gfa",
        #     "p_ctg.gfa",
        #     "p_ctg.lowQ.bed",
        #     "p_ctg.noseq.gfa",
        #     "p_utg.gfa",
        #     "p_utg.lowQ.bed",
        #     "p_utg.noseq.gfa",
        #     "r_utg.gfa",
        #     "r_utg.lowQ.bed",
        #     "r_utg.noseq.gfa",
        # )
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/{prefix}.hifiasm.log"
    params: 
        prefix="RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}",
        loglevel=config["LOGLEVEL"],
        chunk=config["CHUNKS"]
#    resources:
#        mem_mb=1024
    envmodules:
        "hifiasm/0.16.1"
    shell: """
        hifiasm -l2 -t {threads} --primary -o {params.prefix} {input}
    """


# Edit output
# create primary and alternate contigs for "-l2" by folding the file and writing as a fasta file
rule EDIT_HIFIASM_OUTPUT:
    input:
        hifigfa1="RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.p_ctg.gfa",
        hifigfa2="RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.a_ctg.gfa"
    output:
        fasta1="RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.p_ctgl2.fasta",
        fasta2="RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.a_ctgl2.fasta"
    shell: """
        awk '/^S/{{print ">"$2;print $3}}' {input.hifigfa1} | fold > {output.fasta1} 
        awk '/^S/{{print ">"$2;print $3}}' {input.hifigfa2} | fold > {output.fasta2}
    """



# BUSCO Score

# -m: genome mode
# -i: input 
# -o: output folder
# -l: lineage/database - define it in config file
# write a script for busco
#write a rule that downloads the environment and make it a local one
rule RUN_BUSCO:
    input:
        "RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.p_ctgl2.fasta"
    output:
        # prefix="RESULTS/GENOME_ASSEMBLY/BUSCO/{prefix}_",
        # short_json="RESULTS/GENOME_ASSEMBLY/BUSCO/{prefix}_short_summary.json",
        # short_txt="RESULTS/GENOME_ASSEMBLY/BUSCO/{prefix}_short_summary.txt",
        # full_table="RESULTS/GENOME_ASSEMBLY/BUSCO/{prefix}_full_table.tsv",
        touch("RESULTS/GENOME_ASSEMBLY/BUSCO/{prefix}_busco_missing.tsv")
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/{prefix}.busco.log"
    params:
        dataset_dir="busco_downloads",
        out_dir="RESULTS/GENOME_ASSEMBLY/BUSCO/",
        run_name=FILE_PREFIX,
        chunk=config["CHUNKS"],
        lineage=config["LINEAGE"]
    shell: """
        busco -m genome \
        -c {threads} \
        -i {input} \
        -o {params.run_name}
        --download_path {params.dataset_dir} \
        --out_path {params.out_dir} \
        -l {params.lineage}
    """

# Purge erroneous duplications with Purge_dups

# split fasta file where Ns occur

rule SPLIT_AT_Ns:
    input:
        "RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.p_ctgl2.fasta"
    output:
        "RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.p_ctgl2_split.fasta"
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/{prefix}.split_fa.log"
    params:
        loglevel=config["LOGLEVEL"],
        chunk=config["CHUNKS"]
    shell: """
        split_fa {input} > {output} 
    """

# xasm5=asm-to-ref mapping, for ~0.1 sequence divergence (assembly to self)
# -I=split index for every ~NUM input bases
rule MINIMAP2_GENOME_TO_SELF:
    input:
        "RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.p_ctgl2_split.fasta"
    output:
        "RESULTS/GENOME_ASSEMBLY/MINIMAP2/{prefix}.p_ctgl2.genome_split.paf"
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/{prefix}.minimap2a.log"
    params:
        chunk=config["CHUNKS"],
        i_split=config["INDEX_SPLIT"]
    shell: """
        minimap2 -I {params.i_split} -t {threads} -xasm5 -DP {input} {input} > {output} 
    """

rule MINIMAP2_GENOME_TO_READS:
    input:
        splitf="RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.p_ctgl2_split.fasta",
        ccs_reads="DATA/{prefix}.fastq.gz"
    output:
        "RESULTS/GENOME_ASSEMBLY/MINIMAP2/{prefix}.p_ctgl2.reads_split.paf"
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/{prefix}.minimap2b.log"
    params:
        loglevel=config["LOGLEVEL"],
        chunk=config["CHUNKS"],
        i_split=config["INDEX_SPLIT"]
    shell: """
        minimap2 -I {params.i_split} -t {threads} -x map-pb {input.splitf} {input.ccs_reads} > {output} 
    """

#Calculate haploid/diploid coverage threshold and remove haplotype duplicates from assembly

rule PBCSTAT:
    input:
        "RESULTS/GENOME_ASSEMBLY/MINIMAP2/{prefix}.p_ctgl2.reads_split.paf"
    output:
        basecov="RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}/PB.base.cov",
        stat="RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}/PB.stat" #, PB.cov.wig
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/{prefix}.pbcstat.log"
    params:
        loglevel=config["LOGLEVEL"],
        chunk=config["CHUNKS"],
        i_split=config["INDEX_SPLIT"],
        prefix="RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}"
    shell: """
        pbcstat -O {params.prefix} {input} 
    """
# pbcstat -O RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/phophogenome143410 "RESULTS/GENOME_ASSEMBLY/MINIMAP2/phophogenome143410.p_ctgl2.reads_split.paf"
rule CALCUTS:
    input:
        "RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}/PB.stat"
    output:
        "RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}/cutoffs"
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/{prefix}.calcuts.log"
    params:
        loglevel=config["LOGLEVEL"],
        chunk=config["CHUNKS"],
        i_split=config["INDEX_SPLIT"],
        prefix="RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}"
    shell: """
        calcuts {input} > {output} 
    """
# calcuts RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/phophogenome143410/PB.stat > RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/phophogenome143410/cutoffs

rule PURGE_DUPS:
    input:
        basecov="RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}/PB.base.cov",
        cutoffs="RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}/cutoffs",
        genome_paf="RESULTS/GENOME_ASSEMBLY/MINIMAP2/{prefix}.p_ctgl2.genome_split.paf"
    output:
        "RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}_dups.bed"
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/{prefix}.purge_dups.log"
    params:
        loglevel=config["LOGLEVEL"],
        chunk=config["CHUNKS"],
        i_split=config["INDEX_SPLIT"],
        prefix="RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}"
    shell: """
        purge_dups -2 -c {input.basecov} -T {input.cutoffs} {input.genome_paf} > {output} 
    """

rule GET_SEQS:
    input:
        purge_duped="RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}_dups.bed",
        fasta="RESULTS/GENOME_ASSEMBLY/HIFIASM/{prefix}.p_ctgl2.fasta"
    output:
        "RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}_seqs_purged.hap.fa"
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/{prefix}.get_seqs.log"
    params:
        loglevel=config["LOGLEVEL"],
        chunk=config["CHUNKS"],
        i_split=config["INDEX_SPLIT"],
        prefix="RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}_seqs_purged"
    shell: """
        get_seqs -e -p {params.prefix} {input.purge_duped} {input.fasta} 
    """

rule INDEX_SEQS:
    input:
        "RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}_seqs_purged.hap.fa"
    output:
        "RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}_seqs_purged.hap.fa.fai"
    shell: """
        samtools faidx {input}
    """

############  HIC ASSEMBLY  ############

############  HIC SCAFFOLDING  ############

# Map HiC reads to contigs with Salsa2
# the - symbol after samtools is to tell samtools to take the input from the pipe
rule MAP_HIC_R1_TO_ASSEMBLY:
    input:
        fasta=expand("RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}_seqs_purged.hap.fa",prefix=FILE_PREFIX),
        hic1="DATA/{hic}.fastq.gz"
    output:
        expand("RESULTS/GENOME_ASSEMBLY/HIC/RAW/{prefix}_{hic}.bam",prefix=FILE_PREFIX,allow_missing=True)
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/HIC.map_bwa_{hic}.log"
    params:
        chunk=config["CHUNKS"]
    shell: """
        bwa index {input.fasta}
        bwa mem -t24 -B8 {input.fasta} {input.hic1} | samtools view -b - > {output}
    """

# rule MAP_HIC_R2_TO_ASSEMBLY:
#     input:
#         fasta=expand("RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}_seqs_purged.hap.fa", prefix=FILE_PREFIX),
#         hic2="DATA/hic2.fastq.gz"
#     output:
#         rawhic2="RESULTS/GENOME_ASSEMBLY/HIC/RAW/{prefix}_hic2.bam"
#     threads:
#         workflow.cores
#     log:
#         "RESULTS/GENOME_ASSEMBLY/LOG/HIC.map_bwa_r2.log"
#     params:
#         chunk=config["CHUNKS"]
#     shell: """
#         bwa mem -t24 -B8 {input.fasta} {input.hic2} | samtools view -Sb - > {output.rawhic2}
#     """
    
    
# Filter for 5â€™-most alignment for each read, scripts are salsa 2 scripts

rule FILTER_5PRIME_ALIGNMENT_HICR1:
    input:
        rawhic1=expand("RESULTS/GENOME_ASSEMBLY/HIC/RAW/{prefix}_{hic}.bam",prefix=FILE_PREFIX,allow_missing=True)
    output:
        filtrawhic1=expand("RESULTS/GENOME_ASSEMBLY/HIC/FILTERED/{prefix}_{hic}_filtered.bam",prefix=FILE_PREFIX,allow_missing=True)
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/HIC.{hic}.filter_r1.log"
    params:
        chunk=config["CHUNKS"]
    shell: """
        samtools view -h  {input.rawhic1} | perl /share/scientific_bin/salsa2/2.3/bin/filter_five_end.pl | \
        samtools view -@ {threads} -b - > {output.filtrawhic1}
    """

# rule FILTER_5PRIME_ALIGNMENT_HICR2:
#     input:
#         rawhic2="RESULTS/GENOME_ASSEMBLY/HIC/RAW/{hic2file}.bam"
#     output:
#         filtrawhic2="RESULTS/GENOME_ASSEMBLY/HIC/FILTERED/{hic2file}.bam"
#     threads:
#         workflow.cores
#     log:
#         "RESULTS/GENOME_ASSEMBLY/LOG/HIC.{hic2file}.filter_r2.log"
#     params:
#         chunk=config["CHUNKS"]
#     shell: """
#         samtools view -h  {input.rawhic2} | perl /share/scientific_bin/salsa2/2.3/bin/filter_five_end.pl | samtools view -@24 -Sb - > {output.filtrawhic2}
#     """

# Combine read 1 and 2 maps with mapq>10 (!)
# use the script from the arima assembly to combine the hic reads
# uses the .fai index of our assembly to index the hic data
rule COMBINE_HICR1_AND_HICR2:
    input:
        hicfiles=expand("RESULTS/GENOME_ASSEMBLY/HIC/FILTERED/{prefix}_{hic}_filtered.bam",hic=["hic1","hic2"],prefix=FILE_PREFIX),
        faidxfile=expand("RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}_seqs_purged.hap.fa.fai",prefix=FILE_PREFIX)
    output:
        "RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/mapped_hic_reads.bam"
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/HIC.combine.log"
    params:
        chunk=config["CHUNKS"]
    # envmodules:
    #     "/share/scientific_bin/salsa2/2.3"
    shell: """
        module load salsa2/2.3
        perl /share/scientific_bin/salsa2/2.3/bin/two_read_bam_combiner.pl {input.hicfiles} samtools 10 | \
        samtools view -@ {threads} -b -t {input.faidxfile} - > {output}
    """

# replace dedup.sh from salsa2: this is better because it's more snakemake-like
# --RGLB \ -LB	null	Read-Group library
# --RGPL \ -PL	null	Read-Group platform (e.g. ILLUMINA, SOLID)
# --RGPU \ -PU	null	Read-Group platform unit (eg. run barcode)
# --RGSM \ -SM	null	Read-Group sample name
rule ADD_READ_GROUP:
    input:
        "RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/mapped_hic_reads.bam"
    output:
        "RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/mapped_hic_reads_withRG.bam"
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/HIC.combine.log"
    params:
        chunk=config["CHUNKS"],
        prefix=FILE_PREFIX,
        label=SAMPLE_LABEL
    # envmodules:
    #     "/share/scientific_bin/salsa2/2.3"
    shell: """
        picard AddOrReplaceReadGroups \
        -I {input} -O {output} \
        -ID {params.prefix} -LB {params.label} -SM {params.label} -PL ILLUMINA -PU none
    """

# Filter duplicate mappings and sort by name (!)
# dedup.sh missing from salsa2 module on cluster
rule SORT_HIC:
    input:
        "RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/mapped_hic_reads_withRG.bam"
    output:
        sorted="RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/mapped_hic_reads_withRG_sorted.bam"
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/HIC.COMBINED.DEDUPED.samtoolssort1.log"
    params:
        chunk=config["CHUNKS"],
        tempf="RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/TEMP/mapped_hic_reads_withRG_sort.bam.tmp"
    shell: """
        mkdir RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/TEMP/
        samtools sort -@ {threads} -T {params.tempf} -m 1G -O bam -o {output.sorted} {input}
    """
    

rule REMOVE_DUPLICATE_MAPPINGS:
    input:
        sorted="RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/mapped_hic_reads_withRG_sorted.bam"
    output:
        deduped="RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/mapped_hic_reads_withRG_deduped.bam",
        metrics="RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/picard_hic_deduped_metrics.txt"
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/HIC.picdedup.log"
    params:
        tempf="RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/TEMP/"
    shell: """
        #-XX:MaxPermSize=1g -XX:+CMSClassUnloadingEnabled
        picard MarkDuplicates -XX:ParallelGCThreads={threads} -Xmx2g -Djava.io.tmpdir={params.tempf} --REMOVE_DUPLICATES true \
        -I {input.sorted} -O {output.deduped} -M {output.metrics} --TMP_DIR {params.tempf} \
        --ASSUME_SORT_ORDER "coordinate" --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 1024 --SORTING_COLLECTION_SIZE_RATIO 0.1 --MAX_RECORDS_IN_RAM 250000
    """

rule RE_SORT_HIC:
    input:
        deduped="RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/mapped_hic_reads_withRG_deduped.bam"
    output:
        resorted=touch("RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/mapped_hic_reads_withRG_deduped_resorted.bam")
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/HIC.samtoolssort2.log"
    params:
        chunk=config["CHUNKS"],
        tempf="RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/TEMP/"
    shell: """
        samtools sort -@ {threads} -n -T {params.tempf} -m 1G -O bam -o {output.resorted} {input.deduped}
    """

# Convert to bed file

rule BAM2BED_HIC:
    input:
        "RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/mapped_hic_reads_withRG_deduped_resorted.bam"
    output:
        "RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/mapped_hic_reads_withRG_deduped_resorted.bed"
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/HIC.bam2bed.log"
    params:
        chunk=config["CHUNKS"]
    shell: """
        bedtools bamtobed -i {input} > {output}
    """

# Run salsa2
#smk cant download packages when using the environment or call the environment itself because we need to use conda activate to do that
# however, conda isnt loaded on the rule so we have to point to out local installation of conda to use it the environment and the end needs to be 
# installed in our cluster
#run create envs
rule RUN_SALSA2:
    input:
        hic_bed="RESULTS/GENOME_ASSEMBLY/HIC/COMBINED/DEDUPED/mapped_hic_reads_withRG_deduped_resorted.bed",
        fasta=expand("RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}_seqs_purged.hap.fa",prefix=FILE_PREFIX),
        fai=expand("RESULTS/GENOME_ASSEMBLY/PURGE_DUPS/{prefix}_seqs_purged.hap.fa.fai",prefix=FILE_PREFIX)
    output:
        agp="RESULTS/GENOME_ASSEMBLY/HIC/SALSA2/scaffolds_FINAL.agp",
        fastout="RESULTS/GENOME_ASSEMBLY/HIC/SALSA2/scaffolds_FINAL.fasta"
    threads:
        workflow.cores
    log:
        "RESULTS/GENOME_ASSEMBLY/LOG/HIC.salsa2.log"
    params:
        chunk=config["CHUNKS"],
        enzymes=config["ENZYMES"],
        dir="RESULTS/GENOME_ASSEMBLY/HIC/SALSA2/"
    conda:
        "envs/salsa2.yaml"
    shell: """
        #source /share/scientific_bin/anaconda3/2022.05/etc/profile.d/conda.sh
        #conda activate salsa2
        python2 /share/scientific_bin/salsa2/2.3/bin/run_pipeline.py --assembly {input.fasta} --length {input.fai} \
        --enzyme {params.enzymes} --bed {input.hic_bed} --output {params.dir} \
        --clean yes --prnt yes --iter 50 
    """
